{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile stream.py\n",
        "import streamlit as st\n",
        "import get_speakers as gs\n",
        "import transcribe as t\n",
        "import chatgpt as gpt\n",
        "import os\n",
        "import yt_dlp\n",
        "\n",
        "# set the layout wide\n",
        "st.set_page_config(layout = \"wide\")\n",
        "\n",
        "# title of the webpage\n",
        "st.title(\"Hi! We recommend a better conversation for you!\")\n",
        "\n",
        "# get an audio file; .wav are allowed to be uploaded\n",
        "audiofile = st.file_uploader(\"Upload an audio file! For now, only .wav is allowed\", type = [\"wav\"] )\n",
        "\n",
        "# or get a YouTube link\n",
        "url = st.text_input(\"Or copy and paste a YouTube link!\")\n",
        "\n",
        "# when audio file is received\n",
        "if audiofile is not None and len(url) == 0:\n",
        "\n",
        "    # when an audio file is given, show the name of it\n",
        "    st.write(f\"We got \\\"{audiofile.name}\\\" file from you. Will be right back with a better conversation!\")\n",
        "\n",
        "    num_speakers = gs.get_num_speakers(audiofile)\n",
        "\n",
        "    T = t.Transcribe(audiofile, num_speakers, True)\n",
        "    result = T.get_results()\n",
        "\n",
        "    G = gpt.ChatGPT_part(result)\n",
        "    gpt = G.ChatGPT()\n",
        "\n",
        "    with st.container():\n",
        "      col1, col2 = st.columns(2, gap=\"large\")\n",
        "\n",
        "\n",
        "      # output the transcript of the given audio file\n",
        "      with col1:\n",
        "        st.header(\"Transcription of your file\")  \n",
        "        st.write(result)\n",
        "\n",
        "\n",
        "      # get a better conversation transcribtion from ChatGPT\n",
        "      with col2:\n",
        "        st.header(\"Here is a better conversation you may try!\")\n",
        "        st.write(gpt)\n",
        "        \n",
        "\n",
        "        \n",
        "# when a YouTube link is received\n",
        "elif len(url) != 0 and audiofile is None:\n",
        "  st.write(f\"We got the following link: {url}. Will be right back with a better conversation!\")\n",
        "\n",
        "  # extract audio part from the given url(YouTube video)\n",
        "  ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    \"outtmpl\" : 'conversation',\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        " }\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download(url)\n",
        "\n",
        "  num_speakers = gs.get_num_speakers(\"conversation.wav\")\n",
        "  T = t.Transcribe(\"conversation.wav\", num_speakers, False)\n",
        "  result = T.get_results()\n",
        "\n",
        "  G = gpt.ChatGPT_part(result)\n",
        "  gpt = G.ChatGPT()\n",
        "\n",
        "\n",
        "  with st.container():\n",
        "    col1, col2 = st.columns(2, gap=\"large\")\n",
        "\n",
        "    with col1:\n",
        "      st.header(\"Transcription of your link\")\n",
        "      st.write(result)\n",
        "    \n",
        "    with col2:\n",
        "      st.header(\"Here is a better conversation you may try!\")\n",
        "      #st.write(gpt)\n",
        "      st.text(gpt)\n",
        "    "
      ],
      "metadata": {
        "id": "2PsdO4RXA7MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile transcribe.py\n",
        "\n",
        "from get_speakers import get_num_speakers\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding( \n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# I added this codes\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "\n",
        "class Transcribe():\n",
        "    '''\n",
        "    this class is for getting audio and number of speakers from stream.py\n",
        "    given those, transcribe the audio\n",
        "    '''\n",
        "    def __init__(self, audio, num_speakers, is_file):\n",
        "        \n",
        "        if is_file:\n",
        "        # when given an audio file\n",
        "          self.audio = audio\n",
        "          self.num_speakers = num_speakers\n",
        "          self.path = audio.name\n",
        "          self.language = \"any\"\n",
        "          self.model_size = 'large-v2'\n",
        "\n",
        "        else:\n",
        "        # when given a link\n",
        "          self.path = audio\n",
        "          self.num_speakers = num_speakers\n",
        "          self.language = \"any\"\n",
        "          self.model_size = 'large-v2'\n",
        "\n",
        "\n",
        "    # available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']\n",
        "    def load_whisper_model(self):\n",
        "      model = whisper.load_model(self.model_size)\n",
        "      return model\n",
        "\n",
        "    # execute the trascribtion\n",
        "    def execute(self):\n",
        "        model = self.load_whisper_model()\n",
        "        result = model.transcribe(\"conversation.wav\") \n",
        "        segments = result[\"segments\"]\n",
        "        return segments\n",
        "    \n",
        "    def clustering(self):\n",
        "        with contextlib.closing(wave.open(\"conversation.wav\",'r')) as f: \n",
        "          frames = f.getnframes()\n",
        "          rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        audio = Audio()\n",
        "        return audio\n",
        "\n",
        "    def segment_embedding(self, segment):\n",
        "      audio = self.clustering()\n",
        "      start = segment[\"start\"]\n",
        "      # Whisper overshoots the end timestamp in the last segment\n",
        "      #end = min(duration, segment[\"end\"])\n",
        "      with contextlib.closing(wave.open(\"conversation.wav\",'r')) as f: \n",
        "          frames = f.getnframes()\n",
        "          rate = f.getframerate()\n",
        "      duration = frames / float(rate)\n",
        "      end = min(duration, segment[\"end\"])\n",
        "      clip = Segment(start, end)\n",
        "      waveform, sample_rate = audio.crop(self.path, clip)\n",
        "      \n",
        "      return embedding_model(waveform[None])\n",
        "    \n",
        "    def get_results(self):\n",
        "      segments = self.execute()\n",
        "      embeddings = np.zeros(shape=(len(segments), 192))\n",
        "      for i, segment in enumerate(segments):\n",
        "          embeddings[i] = self.segment_embedding(segment)\n",
        "\n",
        "      embeddings = np.nan_to_num(embeddings)\n",
        "\n",
        "      clustering = AgglomerativeClustering(self.num_speakers).fit(embeddings)\n",
        "      labels = clustering.labels_\n",
        "      for i in range(len(segments)):\n",
        "          segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "      result = [ seg[\"speaker\"] + \": \" + seg[\"text\"] for c, seg in enumerate(segments)]\n",
        "      return result"
      ],
      "metadata": {
        "id": "OPLN7OSDEFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatgpt.py\n",
        "import openai\n",
        "openai.api_key = \"your ChatGPT API key here\"\n",
        "\n",
        "class ChatGPT_part():\n",
        "  def __init__(self, transcript):\n",
        "    self.transcript = transcript\n",
        "  \n",
        "  def ChatGPT(self):\n",
        "    content = '''다음의 대화에서 1.화자들이 어떤 관계인지 추측해보고 2.대화에 어떤 갈등이 있는지 추측해봐\n",
        "    3. i-message로 주어진 대화를 개선해봐 4. 개선된 대화를 대화형식으로 반환해줘.'''\n",
        "\n",
        "    for c in self.transcript:\n",
        "      content = content + c + \"\\n\"\n",
        "      \n",
        "    messages = []\n",
        "    messages.append({\"role\": \"user\", \"content\": content})\n",
        "    completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = messages)\n",
        "    chat_response = completion.choices[0].message.content\n",
        "    return(chat_response[chat_response.index(\"4.\") + 3: ])"
      ],
      "metadata": {
        "id": "IIsuSyIwEHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_speakers.py\n",
        "'''\n",
        "add 2 seconds silence to the existing conversation.wav\n",
        "this makes a better diarization\n",
        "'''\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"your huggingface API here\")\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "# authorization key should not be exposed\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"your huggingface API here\")\n",
        "\n",
        "\n",
        "def get_num_speakers(original_audio):\n",
        "    '''\n",
        "    given an audio file,\n",
        "    return the number of speakers in an audio file: original_audio\n",
        "    '''\n",
        "\n",
        "    t1 = 0 * 1000 # Works in milliseconds\n",
        "    t2 = 10 * 60 * 1000 # t1:t2 is total 10mins\n",
        "\n",
        "    newAudio = AudioSegment.from_wav(original_audio)\n",
        "    a = newAudio[t1:t2]\n",
        "    a.export(\"conversation.wav\", format=\"wav\") \n",
        "\n",
        "    audio = AudioSegment.from_wav(\"conversation.wav\")\n",
        "    spacermilli = 2000\n",
        "    spacer = AudioSegment.silent(duration=spacermilli)\n",
        "    audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "    audio.export('audio.wav', format='wav')\n",
        "\n",
        "    # 4. apply pretrained pipeline\n",
        "    diarization = pipeline(\"audio.wav\")\n",
        "\n",
        "    how_many = set()\n",
        "    # 5. print the result\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
        "        how_many.add(int(speaker[-2:]))\n",
        "\n",
        "    # the length of how_many is the total number of speakers in an audio file\n",
        "    #print(len(how_many))\n",
        "\n",
        "    return len(how_many)"
      ],
      "metadata": {
        "id": "sCI77et-A7I_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}