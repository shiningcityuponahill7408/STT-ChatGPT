{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD64MlYruR0l",
        "outputId": "fe31a7be-9680-4da5-873f-720f050d9943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvS-qLW4vM20",
        "outputId": "cf6430a4-4ddb-4f83-e569-72845f8aa10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.987s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYq2DZ21vQAb"
      },
      "outputs": [],
      "source": [
        "!pip install -U yt-dlp -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuuvBC9RvR3c"
      },
      "outputs": [],
      "source": [
        "!wget -O - -q  https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz | xz -qdc| tar -x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EImF6ZUvXsL"
      },
      "outputs": [],
      "source": [
        "!pip install pydub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziaAsvLkvY4U",
        "outputId": "729f21f2-d39c-4380-8874-53050fbae021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.7/390.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.2/217.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyannote.audio==2.1.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXPkPnhBvfTD",
        "outputId": "afce51bf-b0e8-413c-cba4-7761d1b437fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.13.1\n",
            "Uninstalling torch-1.13.1:\n",
            "  Successfully uninstalled torch-1.13.1\n",
            "Found existing installation: torchvision 0.15.1+cu118\n",
            "Uninstalling torchvision-0.15.1+cu118:\n",
            "  Successfully uninstalled torchvision-0.15.1+cu118\n",
            "Found existing installation: torchaudio 0.13.1\n",
            "Uninstalling torchaudio-0.13.1:\n",
            "  Successfully uninstalled torchaudio-0.13.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyannote-audio 2.1.1 requires torchaudio<1.0,>=0.10, but you have torchaudio 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip uninstall --yes torch torchvision torchaudio \n",
        "! pip3 install torch torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klcGyyKRvlRq"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVKS0BBErhSq",
        "outputId": "48f2f51d-3565-4c4b-c2b0-d73f4d0abcbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.4\n"
          ]
        }
      ],
      "source": [
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uthiQssIFs2t",
        "outputId": "ef695232-6b7f-494c-98c8-dc3d3f7a6d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.9/dist-packages (2023.3.4)\n",
            "Requirement already satisfied: mutagen in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (1.46.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (11.0.2)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (1.0.9)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (3.17)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo7cm7ZwHbmk",
        "outputId": "6a7f4a99-09d7-4830-b176-1a8fb2758cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=pL-uy-SkH0w\n",
            "[youtube] pL-uy-SkH0w: Downloading webpage\n",
            "[youtube] pL-uy-SkH0w: Downloading android player API JSON\n",
            "[info] pL-uy-SkH0w: Downloading 1 format(s): 140\n",
            "[dashsegments] Total fragments: 1\n",
            "[download] Destination: conversation\n",
            "[download] 100% of    2.08MiB in 00:00:00 at 9.57MiB/s               \n",
            "[FixupM4a] Correcting container of \"conversation\"\n",
            "[ExtractAudio] Destination: conversation.wav\n",
            "Deleting original file conversation (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "# import yt_dlp\n",
        "\n",
        "# URLS = ['https://www.youtube.com/watch?v=pL-uy-SkH0w']\n",
        "\n",
        "# ydl_opts = {\n",
        "#     'format': 'm4a/bestaudio/best',\n",
        "#     \"outtmpl\" : 'conversation',\n",
        "#     # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "#     'postprocessors': [{  # Extract audio using ffmpeg\n",
        "#         'key': 'FFmpegExtractAudio',\n",
        "#         'preferredcodec': 'wav',\n",
        "#     }]\n",
        "# }\n",
        "\n",
        "# with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "#     error_code = ydl.download(URLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWlfSGlNlj8J",
        "outputId": "83a85173-e14e-4c1a-890e-6db86e55ade1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing chatgpt.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile chatgpt.py\n",
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "class ChatGPT_part():\n",
        "  def __init__(self, transcript):\n",
        "    self.transcript = transcript\n",
        "  \n",
        "  def ChatGPT(self):\n",
        "    content = '''\n",
        "    다음의 대화를 보고 1. 화자들이 어떤 관계인지 추측해보고 2. 화자들이 가지고 있는 갈등이 있는지 추측해봐\n",
        "    3. i-message 개념에 기반해서 개선된 대화로 바꿔줘 \n",
        "    '''\n",
        "\n",
        "    for c in self.transcript:\n",
        "      content = content + c\n",
        "      \n",
        "    messages = []\n",
        "    messages.append({\"role\": \"user\", \"content\": content})\n",
        "    completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", messages = messages)\n",
        "    chat_response = completion.choices[0].message.content\n",
        "    return(chat_response[chat_response.index(\"3.\") + 3: ])\n",
        "\n",
        "#print(f\"ChatGPT: {chat_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh-74MZvvn87",
        "outputId": "c6250d71-794c-40f4-d77e-716a17031c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing stream.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile stream.py\n",
        "import streamlit as st\n",
        "import get_speakers as gs\n",
        "import transcribe as t\n",
        "import chatgpt as gpt\n",
        "import os\n",
        "import yt_dlp\n",
        "\n",
        "# set the layout wide\n",
        "st.set_page_config(layout = \"wide\")\n",
        "\n",
        "# visual components\n",
        "st.title(\"Hi! We recommend a better conversation for you!\")\n",
        "\n",
        "# get an audio file; wav and mp3 are allowed to be uploaded\n",
        "audiofile = st.file_uploader(\"Upload an audio file! You can upload .wav or .mp3\", type = [\"wav\"] )\n",
        "\n",
        "# or get a YouTube link\n",
        "url = st.text_input(\"Or copy and paste a YouTube link!\")\n",
        "\n",
        "# when audio file is received\n",
        "if audiofile is not None and len(url) == 0:\n",
        "\n",
        "    # when an audio file is given, show the name of it\n",
        "    st.write(f\"we got \\\"{audiofile.name}\\\" file from you. Will be right back with a better conversation!\")\n",
        "\n",
        "    num_speakers = gs.get_num_speakers(audiofile)\n",
        "\n",
        "    T = t.Transcribe(audiofile, num_speakers, True)\n",
        "    result = T.get_results()\n",
        "\n",
        "    G = gpt.ChatGPT_part(result)\n",
        "    gpt = G.ChatGPT()\n",
        "\n",
        "    with st.container():\n",
        "      col1, col2 = st.columns(2, gap=\"large\")\n",
        "\n",
        "\n",
        "      # output the transcript of the given audio file\n",
        "      with col1:\n",
        "        st.header(\"Transcription of your file\")  \n",
        "        st.write(result)\n",
        "\n",
        "\n",
        "      # get a better conversation transcribtion from ChatGPT\n",
        "      with col2:\n",
        "        st.header(\"Here is a better conversation you may try!\")\n",
        "        st.write(gpt)\n",
        "        \n",
        "\n",
        "        \n",
        "# when a YouTube link is received\n",
        "elif len(url) != 0 and audiofile is None:\n",
        "  st.write(f\"we got the following link: {url}. Will be right back with a better conversation!\")\n",
        "\n",
        "  # extract audio part from the given url(YouTube video)\n",
        "  ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    \"outtmpl\" : 'conversation',\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        " }\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download(url)\n",
        "\n",
        "  num_speakers = gs.get_num_speakers(\"conversation.wav\")\n",
        "  T = t.Transcribe(\"conversation.wav\", num_speakers, False)\n",
        "  result = T.get_results()\n",
        "\n",
        "  G = gpt.ChatGPT_part(result)\n",
        "  gpt = G.ChatGPT()\n",
        "\n",
        "\n",
        "  with st.container():\n",
        "    col1, col2 = st.columns(2, gap=\"large\")\n",
        "\n",
        "    with col1:\n",
        "      st.header(\"Transcription of your link\")\n",
        "      st.write(result)\n",
        "    \n",
        "    with col2:\n",
        "      st.header(\"Here is a better conversation you may try!\")\n",
        "      st.write(gpt)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpz6N93HvrDM",
        "outputId": "a8e1e518-afc7-4fb8-ff23-500b8add387d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing get_speakers.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_speakers.py\n",
        "'''\n",
        "add 2 seconds silence to the existing conversation.wav\n",
        "this makes a better diarization\n",
        "'''\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"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\")\n",
        "\n",
        "from pydub import AudioSegment\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "# authorization key should not be exposed\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=\"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\")\n",
        "\n",
        "\n",
        "def get_num_speakers(original_audio):\n",
        "    '''\n",
        "    given an audio file,\n",
        "    return the number of speakers in an audio file: original_audio\n",
        "    '''\n",
        "\n",
        "    t1 = 0 * 1000 # Works in milliseconds\n",
        "    t2 = 10 * 60 * 1000 # t1:t2 is total 10mins\n",
        "\n",
        "    newAudio = AudioSegment.from_wav(original_audio)\n",
        "    a = newAudio[t1:t2]\n",
        "    a.export(\"conversation.wav\", format=\"wav\") \n",
        "\n",
        "    audio = AudioSegment.from_wav(\"conversation.wav\")\n",
        "    spacermilli = 2000\n",
        "    spacer = AudioSegment.silent(duration=spacermilli)\n",
        "    audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "    audio.export('audio.wav', format='wav')\n",
        "\n",
        "    # 4. apply pretrained pipeline\n",
        "    diarization = pipeline(\"audio.wav\")\n",
        "\n",
        "    how_many = set()\n",
        "    # 5. print the result\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
        "        how_many.add(int(speaker[-2:]))\n",
        "\n",
        "    # the length of how_many is the total number of speakers in an audio file\n",
        "    #print(len(how_many))\n",
        "\n",
        "    return len(how_many)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RArS_fYTvtZ8",
        "outputId": "a2ecc93e-6fe4-4a3f-e08e-d0f308d3d4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile transcribe.py\n",
        "\n",
        "from get_speakers import get_num_speakers\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding( \n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# I added this codes\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "import pywhisper\n",
        "\n",
        "\n",
        "'''\n",
        "여기에서는 음성파일을 받아서\n",
        "get_speakers.py 로 부터는 음성파일 속에 몇명이 있는지를 받아서\n",
        "대화 전사를 하는 파일\n",
        "'''\n",
        "\n",
        "\n",
        "class Transcribe():\n",
        "    '''\n",
        "    this class is for getting audio and number of speakers from stream.py\n",
        "    given those, transcribe the audio\n",
        "    '''\n",
        "    def __init__(self, audio, num_speakers, is_file):\n",
        "        \n",
        "        if is_file:\n",
        "        # when given an audio file\n",
        "          self.audio = audio\n",
        "          self.num_speakers = num_speakers\n",
        "          self.path = audio.name\n",
        "          self.language = \"any\"\n",
        "          self.model_size = 'large'\n",
        "\n",
        "        else:\n",
        "        # when given a link\n",
        "          self.path = audio\n",
        "          self.num_speakers = num_speakers\n",
        "          self.language = \"any\"\n",
        "          self.model_size = 'large'\n",
        "\n",
        "\n",
        "    # available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']\n",
        "    def load_whisper_model(self):\n",
        "      model = whisper.load_model(self.model_size)\n",
        "      return model\n",
        "\n",
        "    # execute the trascribtion\n",
        "    def execute(self):\n",
        "        model = self.load_whisper_model()\n",
        "        result = model.transcribe(\"conversation.wav\") ########self.path\n",
        "        segments = result[\"segments\"]\n",
        "        return segments\n",
        "    \n",
        "    def clustering(self):\n",
        "        with contextlib.closing(wave.open(\"conversation.wav\",'r')) as f: #self.path\n",
        "          frames = f.getnframes()\n",
        "          rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        audio = Audio()\n",
        "        return audio\n",
        "\n",
        "    def segment_embedding(self, segment):\n",
        "      audio = self.clustering()\n",
        "      start = segment[\"start\"]\n",
        "      # Whisper overshoots the end timestamp in the last segment\n",
        "      #end = min(duration, segment[\"end\"])\n",
        "      with contextlib.closing(wave.open(\"conversation.wav\",'r')) as f: #self.path\n",
        "          frames = f.getnframes()\n",
        "          rate = f.getframerate()\n",
        "      duration = frames / float(rate)\n",
        "      end = min(duration, segment[\"end\"])\n",
        "      clip = Segment(start, end)\n",
        "      waveform, sample_rate = audio.crop(self.path, clip)\n",
        "      \n",
        "      return embedding_model(waveform[None])\n",
        "    \n",
        "    def get_results(self):\n",
        "      segments = self.execute()\n",
        "      embeddings = np.zeros(shape=(len(segments), 192))\n",
        "      for i, segment in enumerate(segments):\n",
        "          embeddings[i] = self.segment_embedding(segment)\n",
        "\n",
        "      embeddings = np.nan_to_num(embeddings)\n",
        "\n",
        "      clustering = AgglomerativeClustering(self.num_speakers).fit(embeddings)\n",
        "      labels = clustering.labels_\n",
        "      for i in range(len(segments)):\n",
        "          segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "      result = [ seg[\"speaker\"] + \": \" + seg[\"text\"] for c, seg in enumerate(segments)]\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tza1olKvvDr"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/stream.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuLApXfyvwRK",
        "outputId": "7470e176-0918-444a-b8c6-2049d288ac60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 7.266s\n",
            "your url is: https://rotten-candles-heal-35-229-46-132.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hL1CrMbNWZv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}